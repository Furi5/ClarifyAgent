"""Clarifier module for assessing information sufficiency and generating clarifications."""
import json
import re
import logging
from typing import Optional
from agents import Agent, Runner
from agents.extensions.models.litellm_model import LitellmModel

from .schema import Plan
from .tools.serperapi import web_search

logger = logging.getLogger(__name__)

# è½»é‡æœç´¢é…ç½®
LIGHT_SEARCH_NUM_RESULTS = 3  # è½»é‡æœç´¢è¿”å›ç»“æœæ•°
SEARCH_CONFIDENCE_MIN = 0.3   # ä½äºæ­¤å€¼ä¸æœç´¢ï¼ˆå¤ªæ¨¡ç³Šï¼‰
SEARCH_CONFIDENCE_MAX = 0.75  # é«˜äºæ­¤å€¼ä¸æœç´¢ï¼ˆå·²è¶³å¤Ÿæ¸…æ™°ï¼‰

# ä¸“ä¸šæœ¯è¯­æ¨¡å¼ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢éªŒè¯ï¼‰
# æ³¨æ„ï¼šä¸ä½¿ç”¨ \b å› ä¸ºå®ƒåœ¨ä¸­æ–‡ç¯å¢ƒä¸­ä¸å·¥ä½œ
DOMAIN_TERM_PATTERNS = [
    r'[A-Z]{2,}[\-]?[A-Z0-9]*',        # å¦‚ KRAS, GLP-1, STAT6
    r'[A-Z][a-z]+[A-Z][a-z]+',          # å¦‚ PembrolizumAb (é©¼å³°å¼)
    r'\w+mab|\w+nib|\w+ide',            # æŠ—ä½“/å°åˆ†å­åç¼€
    r'PD-?[L]?1|CTLA-?4',              # å…ç–«æ£€æŸ¥ç‚¹
    r'STAT\d|JAK\d',                    # ä¿¡å·é€šè·¯
    r'GLP-?\d',                         # GLP-1 ç­‰
]


CLARIFIER_SYSTEM_PROMPT = """\
You are a clarification module for a Deep Research platform focused on drug design and biomedical research.
You MUST output ONLY valid JSON (no markdown, no extra text).

Your role:
- Assess if user input has enough information to start research
- Generate clarification questions when information is insufficient
- Provide options rather than open-ended questions
- Infer research focus when possible
- **Use pre-search context (if provided) to generate more informed options**

## Using Pre-Search Context

When `search_context` is provided in the input:
1. Use it to verify domain terms and understand the research area
2. Generate clarification options based on real-world data from search results
3. Increase confidence if search results confirm the topic is valid and well-defined
4. Use search insights to suggest relevant research focus areas

Example: If user mentions "STAT6 inhibitor" and search shows it's related to immune diseases:
- Use this to provide specific indication options (asthma, atopic dermatitis, etc.)
- Don't ask generic questions like "what do you want to research?"

## Decision Logic

1. If confidence < 0.6:
   â†’ NEED_CLARIFICATION (must clarify - topic unclear or missing)

2. If 0.6 <= confidence < 0.7 and missing key information:
   â†’ NEED_CLARIFICATION (clarify missing info - research focus unclear)

3. If 0.7 <= confidence < 0.85:
   â†’ CONFIRM_PLAN (show inferred plan, let user confirm)

4. If confidence >= 0.85:
   â†’ START_RESEARCH (sufficient information, can start directly)

5. If unknown term detected:
   â†’ VERIFY_TOPIC (search to verify before proceeding)

## Assessment Criteria

Evaluate these dimensions (each contributes to confidence):

1. **Research topic clarity** (0.0-0.3):
   - Clear subject? (e.g., "KRAS G12C", "GLP-1 agonist") â†’ +0.3
   - Vague subject? (e.g., "that drug", "something") â†’ +0.0
   - Missing subject? â†’ +0.0

2. **Research scope inferability** (0.0-0.3):
   - Can infer 3+ research focus areas? â†’ +0.3
   - Can infer 1-2 focus areas? â†’ +0.15
   - Cannot infer focus? â†’ +0.0

3. **Research goal clarity** (0.0-0.2):
   - Clear goal? (e.g., "latest progress", "mechanism", "clinical data") â†’ +0.2
   - Implied goal? â†’ +0.1
   - No goal? â†’ +0.0

4. **Key term understanding** (0.0-0.2):
   - All terms understood? â†’ +0.2
   - Some terms unclear? â†’ +0.1
   - Unknown terms? â†’ VERIFY_TOPIC

## Clarification Principles

1. **Only ask what affects research direction**
   - âŒ Don't ask: "What do you want to research?" (too broad)
   - âœ… Ask: "Which aspect of KRAS G12C? A) Target validation B) Approved drugs C) Clinical trials"

2. **Provide 3-5 options**
   - Options should be mutually exclusive
   - Last option: "Other (please specify)"
   - Based on domain knowledge

3. **One clarification at a time**
   - Focus on the most critical missing information
   - Don't overwhelm user

## Examples

### Example 1: Sufficient Information
Input: "KRAS G12C é¶ç‚¹"
Assessment:
- Topic clarity: 0.3 (clear)
- Scope inferability: 0.3 (can infer: validation, drugs, trials, resistance)
- Goal clarity: 0.1 (implied: general research)
- Term understanding: 0.2 (all understood)
- Confidence: 0.9
â†’ START_RESEARCH

### Example 2: Need Clarification
Input: "å¸®æˆ‘ç ”ç©¶ä¸€ä¸‹"
Assessment:
- Topic clarity: 0.0 (missing)
- Scope inferability: 0.0 (cannot infer)
- Goal clarity: 0.0 (no goal)
- Confidence: 0.0
â†’ NEED_CLARIFICATION
{
    "clarification": {
        "question": "è¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³ç ”ç©¶çš„å…·ä½“ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ",
        "options": [
            "ç‰¹å®šé¶ç‚¹æˆ–åŸºå› ï¼ˆå¦‚ KRAS G12Cï¼‰",
            "ç‰¹å®šè¯ç‰©æˆ–åŒ–åˆç‰©ï¼ˆå¦‚ GLP-1 æ¿€åŠ¨å‰‚ï¼‰",
            "ç‰¹å®šç–¾ç—…æˆ–é€‚åº”ç—‡ï¼ˆå¦‚ 2å‹ç³–å°¿ç—…ï¼‰",
            "ç‰¹å®šæŠ€æœ¯æˆ–æ–¹æ³•ï¼ˆå¦‚ PROTAC è®¾è®¡ï¼‰",
            "å…¶ä»–ï¼ˆè¯·è¯´æ˜ï¼‰"
        ],
        "missing_info": "research_topic"
    }
}

### Example 3: Confirm Plan
Input: "GLP-1 æ¿€åŠ¨å‰‚"
Assessment:
- Topic clarity: 0.3 (clear)
- Scope inferability: 0.3 (can infer focus)
- Goal clarity: 0.1 (implied)
- Confidence: 0.75
â†’ CONFIRM_PLAN

## Output Format

{
    "next_action": "START_RESEARCH|NEED_CLARIFICATION|CONFIRM_PLAN|VERIFY_TOPIC",
    "task": {
        "goal": "Research goal (inferred or from user)",
        "research_focus": ["focus 1", "focus 2", ...]  # At least 3 for START_RESEARCH
    },
    "confidence": 0.0-1.0,
    "why": "Brief assessment reason",
    "clarification": {
        "question": "Clear question",
        "options": ["Option 1", "Option 2", ...],
        "missing_info": "research_topic|research_focus|research_goal"
    },  # Only if NEED_CLARIFICATION
    "assumptions": ["Assumption 1", ...],  # If any assumptions made
    "confirm_prompt": "Prompt for confirmation"  # If CONFIRM_PLAN
}
"""


def build_clarifier(model: LitellmModel) -> Agent:
    """Build the clarifier agent."""
    return Agent(
        name="Clarifier",
        model=model,
        instructions=CLARIFIER_SYSTEM_PROMPT,
        tools=[]  # Clarifier doesn't use tools
    )


def extract_domain_terms(text: str) -> list[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–ä¸“ä¸šæœ¯è¯­ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢éªŒè¯ï¼‰ã€‚
    
    Args:
        text: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
    
    Returns:
        æå–çš„ä¸“ä¸šæœ¯è¯­åˆ—è¡¨
    """
    terms = set()
    for pattern in DOMAIN_TERM_PATTERNS:
        matches = re.findall(pattern, text, re.IGNORECASE)
        terms.update(m.strip() for m in matches if len(m.strip()) >= 2)
    
    # è¿‡æ»¤å¸¸è§è¯
    common_words = {'AI', 'OK', 'API', 'US', 'UK', 'EU', 'IT', 'ID'}
    terms = [t for t in terms if t.upper() not in common_words]
    
    return list(terms)[:5]  # æœ€å¤šè¿”å›5ä¸ª


def build_search_query(user_input: str, terms: list[str]) -> str:
    """
    æ„å»ºè½»é‡æœç´¢çš„æŸ¥è¯¢è¯­å¥ã€‚
    
    Args:
        user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
        terms: æå–çš„ä¸“ä¸šæœ¯è¯­
    
    Returns:
        æœç´¢æŸ¥è¯¢è¯­å¥
    """
    if terms:
        # ä½¿ç”¨ä¸“ä¸šæœ¯è¯­æ„å»ºæŸ¥è¯¢
        main_term = terms[0]
        return f"{main_term} drug research indications mechanism"
    else:
        # æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ ¸å¿ƒè¯
        # å»æ‰å¸¸è§è¯·æ±‚è¯
        cleaned = re.sub(r'(å¸®æˆ‘|è¯·|æœé›†|ç ”ç©¶|åˆ†æ|äº†è§£|æŸ¥æ‰¾|æ‰¾)', '', user_input)
        cleaned = cleaned.strip()[:50]  # é™åˆ¶é•¿åº¦
        if cleaned:
            return f"{cleaned} research overview"
    return ""


async def pre_clarification_search(
    user_input: str,
    terms: list[str],
    num_results: int = LIGHT_SEARCH_NUM_RESULTS
) -> Optional[dict]:
    """
    æ¾„æ¸…å‰çš„è½»é‡æœç´¢ï¼Œè·å–èƒŒæ™¯ä¿¡æ¯ã€‚
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        terms: ä¸“ä¸šæœ¯è¯­åˆ—è¡¨
        num_results: æœç´¢ç»“æœæ•°é‡
    
    Returns:
        æœç´¢ç»“æœå­—å…¸ï¼ŒåŒ…å« domain_context å’Œ verified_terms
    """
    query = build_search_query(user_input, terms)
    if not query:
        return None
    
    try:
        logger.info(f"ğŸ” è½»é‡æœç´¢: {query}")
        search_result = await web_search(query, num_results=num_results)
        
        return {
            "query": query,
            "domain_context": search_result,
            "verified_terms": terms,
            "has_results": bool(search_result and "æœªæ‰¾åˆ°" not in search_result)
        }
    except Exception as e:
        logger.warning(f"è½»é‡æœç´¢å¤±è´¥: {e}")
        return None


def should_do_pre_search(user_input: str, task_draft: dict) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œæ¾„æ¸…å‰æœç´¢ã€‚
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        task_draft: å½“å‰ä»»åŠ¡è‰ç¨¿
    
    Returns:
        True å¦‚æœéœ€è¦æœç´¢
    """
    # å¦‚æœä»»åŠ¡è‰ç¨¿å·²æœ‰æ˜ç¡®ç›®æ ‡ï¼Œä¸éœ€è¦æœç´¢
    if task_draft and task_draft.get("goal") and task_draft.get("research_focus"):
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸“ä¸šæœ¯è¯­
    terms = extract_domain_terms(user_input)
    if terms:
        return True
    
    # æ£€æŸ¥è¾“å…¥é•¿åº¦ - å¤ªçŸ­ä¸æœç´¢
    if len(user_input.strip()) < 10:
        return False
    
    # åŒ…å«ç ”ç©¶ç›¸å…³å…³é”®è¯
    research_keywords = ['ç ”ç©¶', 'è°ƒç ”', 'åˆ†æ', 'æœºåˆ¶', 'é¶ç‚¹', 'è¯ç‰©', 'ä¸´åºŠ', 'å¸‚åœº', 'ç«äº‰']
    if any(kw in user_input for kw in research_keywords):
        return True
    
    return False


def _extract_json(s: str) -> dict:
    """Extract JSON from agent output."""
    s = (s or "").strip()
    if s.startswith("{") and s.endswith("}"):
        return json.loads(s)
    a, b = s.find("{"), s.rfind("}")
    if a != -1 and b != -1 and b > a:
        return json.loads(s[a:b+1])
    raise ValueError(f"Clarifier did not return JSON: {s[:200]}")


def should_clarify(plan: Plan) -> bool:
    """
    Determine if clarification is needed based on plan.
    
    Args:
        plan: Plan from clarifier
    
    Returns:
        True if clarification is needed
    """
    # Hard boundary: must clarify
    if plan.confidence < 0.6:
        return True
    
    # Soft boundary: missing key info
    if 0.6 <= plan.confidence < 0.7:
        # Check if research_focus is missing or too few
        if not plan.task.research_focus or len(plan.task.research_focus) < 2:
            return True
    
    return False


def should_start_research(plan: Plan) -> bool:
    """
    Determine if research can start directly.
    
    Args:
        plan: Plan from clarifier
    
    Returns:
        True if can start research directly
    """
    return (
        plan.confidence >= 0.85 and
        plan.task.goal and
        len(plan.task.research_focus) >= 3 and
        plan.next_action != "VERIFY_TOPIC"
    )


async def assess_input(
    model: LitellmModel,
    messages: list[dict],
    task_draft: dict,
    enable_pre_search: bool = True
) -> Plan:
    """
    Assess user input and determine if clarification is needed.
    
    Args:
        model: LLM model for clarification
        messages: Conversation history
        task_draft: Current task draft
        enable_pre_search: Whether to enable pre-clarification search
    
    Returns:
        Plan with next_action and assessment
    """
    clarifier = build_clarifier(model)
    
    # Build context from messages
    context = ""
    user_input = ""
    if messages:
        # Get last few messages for context
        recent_messages = messages[-3:] if len(messages) > 3 else messages
        context = "\n".join([
            f"{msg.get('role', 'user')}: {msg.get('content', '')}"
            for msg in recent_messages
        ])
        # Get latest user input
        for msg in reversed(messages):
            if msg.get('role') == 'user':
                user_input = msg.get('content', '')
                break
    
    # Pre-clarification search (if enabled and appropriate)
    search_context = None
    if enable_pre_search and user_input and should_do_pre_search(user_input, task_draft):
        terms = extract_domain_terms(user_input)
        search_context = await pre_clarification_search(user_input, terms)
        if search_context:
            logger.info(f"âœ… è½»é‡æœç´¢å®Œæˆï¼Œå‘ç°æœ¯è¯­: {search_context.get('verified_terms', [])}")
    
    payload = {
        "messages": messages,
        "task_draft": task_draft or {},
        "context": context,
        "search_context": search_context,  # æ·»åŠ æœç´¢ä¸Šä¸‹æ–‡
        "schema_hint": {
            "next_action": "START_RESEARCH|NEED_CLARIFICATION|CONFIRM_PLAN|VERIFY_TOPIC",
            "task": {
                "goal": "string",
                "research_focus": ["string"]  # At least 3 for START_RESEARCH
            },
            "confidence": "0.0-1.0",
            "why": "string",
            "clarification": {
                "question": "string",
                "options": ["string"],
                "missing_info": "research_topic|research_focus|research_goal"
            },
            "assumptions": ["string"],
            "confirm_prompt": "string"
        }
    }
    
    result = await Runner.run(clarifier, json.dumps(payload, ensure_ascii=False))
    data = _extract_json(result.final_output or "")
    plan = Plan.model_validate(data)
    
    # Post-process: enforce decision logic
    if should_clarify(plan) and plan.next_action not in ["NEED_CLARIFICATION", "VERIFY_TOPIC"]:
        plan.next_action = "NEED_CLARIFICATION"
    
    if should_start_research(plan) and plan.next_action == "CONFIRM_PLAN":
        plan.next_action = "START_RESEARCH"
    
    return plan
