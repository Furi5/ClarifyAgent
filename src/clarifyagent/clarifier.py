"""Clarifier module for assessing information sufficiency and generating clarifications."""
import json
import re
import logging
from typing import Optional
from agents import Agent, Runner
from agents.extensions.models.litellm_model import LitellmModel

from .schema import Plan
from .tools.serperapi import web_search

logger = logging.getLogger(__name__)

# è½»é‡æœç´¢é…ç½®
LIGHT_SEARCH_NUM_RESULTS = 3  # è½»é‡æœç´¢è¿”å›ç»“æœæ•°
SEARCH_CONFIDENCE_MIN = 0.3   # ä½äºæ­¤å€¼ä¸æœç´¢ï¼ˆå¤ªæ¨¡ç³Šï¼‰
SEARCH_CONFIDENCE_MAX = 0.75  # é«˜äºæ­¤å€¼ä¸æœç´¢ï¼ˆå·²è¶³å¤Ÿæ¸…æ™°ï¼‰

# ä¸“ä¸šæœ¯è¯­æ¨¡å¼ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢éªŒè¯ï¼‰
# é€šç”¨æ¨¡å¼ï¼Œé€‚ç”¨äºå„é¢†åŸŸ
DOMAIN_TERM_PATTERNS = [
    r'[A-Z]{2,}[\-]?[A-Z0-9]*',        # ç¼©å†™è¯ï¼šå¦‚ AI, API, GDP, ESG
    r'[A-Z][a-z]+[A-Z][a-z]+',          # é©¼å³°å¼ä¸“æœ‰åè¯
    r'[A-Z][a-z]+(?:\s[A-Z][a-z]+)+',   # å¤šè¯ä¸“æœ‰åè¯ï¼šå¦‚ Tesla Model
]


CLARIFIER_SYSTEM_PROMPT = """\
You are a clarification module for a Deep Research platform.
You MUST output ONLY valid JSON (no markdown, no extra text).

## Core Principle: Minimize User Friction

**Ask as few questions as possible. One well-crafted question is better than five.**

## CRITICAL: Understanding Conversation Context

When processing user input, ALWAYS check:
1. **conversation_summary** - Shows the original request and any follow-up answers
2. **task_draft.pipeline_info** - Contains user-provided project/product details
3. **task_draft.clarification_responses** - Previous Q&A pairs

**Example flow:**
- User first says: "è¯„ä¼°æˆ‘ä»¬çš„äº§å“"
- System asks: "è¯·ç®€å•æè¿°æ‚¨çš„äº§å“ï¼šåç§°ã€ç±»å‹ã€ç›®æ ‡å¸‚åœºæ˜¯ä»€ä¹ˆï¼Ÿ"
- User answers: "æ™ºèƒ½éŸ³ç®±ï¼Œé¢å‘å®¶åº­ç”¨æˆ·"
- NOW: conversation_summary will show this is a FOLLOW-UP answer
- DO NOT ask more questions - user has provided the info, proceed to research!

**If user has already provided project details, confidence should be HIGH (0.85+)**

## Detecting Private vs Public Information

**CRITICAL**: Distinguish between:
1. **Public information**: Named entities that can be searched (companies, products, people, events)
   â†’ Can directly research with web search
2. **Private information**: User's own project/product/data (e.g., "æˆ‘ä»¬çš„äº§å“", "æˆ‘çš„é¡¹ç›®", "å…¬å¸çš„æ–¹æ¡ˆ")
   â†’ Must ask user to provide details

**Signals of private information:**
- "æˆ‘ä»¬çš„", "æˆ‘çš„", "å…¬å¸çš„", "å›¢é˜Ÿçš„"
- "æˆ‘æœ‰ä¸€ä¸ª", "æˆ‘ä»¬å¼€å‘çš„", "è‡ªç ”çš„"
- "our", "my", "we have"

## Clarification Strategy

### For Private Information â†’ Use ONE Open-Ended Question

When user mentions their own project/product, ask ONE comprehensive question to gather key details:

```json
{
    "clarification": {
        "question": "è¯·ç®€å•æè¿°æ‚¨çš„é¡¹ç›®/äº§å“ï¼šå…·ä½“æ˜¯ä»€ä¹ˆï¼Ÿç›®å‰å¤„äºä»€ä¹ˆé˜¶æ®µï¼Ÿä¸»è¦ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ",
        "options": [],
        "missing_info": "project_details",
        "open_ended": true
    }
}
```

**DO NOT** ask multiple rounds. **DO** ask once with all key questions.

### For Public Information â†’ Smart Defaults

If topic is clear and searchable, **don't ask** unnecessary questions.
Assume comprehensive research and directly start.

Only clarify if truly ambiguous (e.g., "å¸®æˆ‘ç ”ç©¶ä¸€ä¸‹" with no context).

### For Ambiguous Requests â†’ Maximum 3 Options

If must provide options, limit to 3:
```json
{
    "options": [
        "é€‰é¡¹1ï¼ˆæœ€å¯èƒ½çš„æ„å›¾ï¼‰",
        "é€‰é¡¹2ï¼ˆæ¬¡å¯èƒ½çš„æ„å›¾ï¼‰", 
        "å…¶ä»–ï¼ˆè¯·è¯´æ˜ï¼‰"
    ]
}
```

## Decision Logic

1. **Private info detected + missing details** â†’ NEED_CLARIFICATION (open-ended)
2. **Public info + clear topic** (confidence >= 0.7) â†’ START_RESEARCH or CONFIRM_PLAN
3. **Completely vague** (confidence < 0.5) â†’ NEED_CLARIFICATION (max 3 options)
4. **Unknown term** â†’ VERIFY_TOPIC

## Assessment Criteria

1. **Topic clarity** (0.0-0.4):
   - Specific named entity â†’ +0.4
   - General category â†’ +0.2
   - Vague/missing â†’ +0.0

2. **Scope inferability** (0.0-0.3):
   - Can infer comprehensive research areas â†’ +0.3
   - Partial inference â†’ +0.15

3. **Goal clarity** (0.0-0.3):
   - Clear goal stated â†’ +0.3
   - Implied goal â†’ +0.2
   - No goal (assume comprehensive) â†’ +0.1

## Examples

### Example 1: Private Information (Open-Ended)
Input: "è¯„ä¼°æˆ‘ä»¬çš„æ–°äº§å“å¸‚åœºå‰æ™¯"
Assessment: Private info (æˆ‘ä»¬çš„), need product details
â†’ NEED_CLARIFICATION
```json
{
    "next_action": "NEED_CLARIFICATION",
    "confidence": 0.3,
    "why": "ç”¨æˆ·æåˆ°'æˆ‘ä»¬çš„'äº§å“ï¼Œéœ€è¦äº†è§£å…·ä½“ä¿¡æ¯",
    "clarification": {
        "question": "è¯·ç®€å•æè¿°æ‚¨çš„äº§å“ï¼šæ˜¯ä»€ä¹ˆç±»å‹çš„äº§å“ï¼Ÿç›®æ ‡ç”¨æˆ·ç¾¤ä½“æ˜¯è°ï¼Ÿä¸»è¦åŠŸèƒ½æˆ–ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "options": [],
        "missing_info": "project_details",
        "open_ended": true
    }
}
```

### Example 2: Public Information (Direct Start)
Input: "ç‰¹æ–¯æ‹‰ 2024 å¹´é”€é‡åˆ†æ"
Assessment: Clear public topic, can infer research scope
â†’ START_RESEARCH (confidence 0.85+)

### Example 3: Ambiguous (Minimal Options)
Input: "å¸®æˆ‘ç ”ç©¶ä¸€ä¸‹å¸‚åœº"
Assessment: Too broad, need to narrow down
â†’ NEED_CLARIFICATION
```json
{
    "clarification": {
        "question": "æ‚¨æƒ³ç ”ç©¶å“ªä¸ªå¸‚åœºï¼Ÿ",
        "options": [
            "ç‰¹å®šè¡Œä¸šå¸‚åœºï¼ˆå¦‚æ–°èƒ½æºã€AIã€åŒ»ç–—ç­‰ï¼‰",
            "ç‰¹å®šåœ°åŒºå¸‚åœºï¼ˆå¦‚ä¸­å›½ã€ç¾å›½ã€ä¸œå—äºšç­‰ï¼‰",
            "å…¶ä»–ï¼ˆè¯·è¯´æ˜å…·ä½“å¸‚åœºï¼‰"
        ],
        "missing_info": "research_scope"
    }
}
```

### Example 4: User Provides Details After Open Question
Previous: Asked for project details
Input: "æ™ºèƒ½å®¶å±…äº§å“ï¼Œé¢å‘å¹´è½»å®¶åº­ï¼Œä¸»æ‰“è¯­éŸ³æ§åˆ¶"
Assessment: All key info provided
â†’ START_RESEARCH (confidence 0.9)

## Output Format

{
    "next_action": "START_RESEARCH|NEED_CLARIFICATION|CONFIRM_PLAN|VERIFY_TOPIC",
    "task": {
        "goal": "Research goal",
        "research_focus": ["focus 1", "focus 2", ...]
    },
    "confidence": 0.0-1.0,
    "why": "Brief reason",
    "clarification": {
        "question": "Question text",
        "options": ["opt1", "opt2", "opt3"],  // Max 3, or empty for open-ended
        "missing_info": "project_details|research_scope|research_topic",
        "open_ended": true|false  // True = no options, user types freely
    },
    "assumptions": ["assumption 1", ...],
    "confirm_prompt": "Confirmation prompt"
}
"""


def build_clarifier(model: LitellmModel) -> Agent:
    """Build the clarifier agent."""
    return Agent(
        name="Clarifier",
        model=model,
        instructions=CLARIFIER_SYSTEM_PROMPT,
        tools=[]  # Clarifier doesn't use tools
    )


def extract_domain_terms(text: str) -> list[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–ä¸“ä¸šæœ¯è¯­ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢éªŒè¯ï¼‰ã€‚
    
    Args:
        text: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
    
    Returns:
        æå–çš„ä¸“ä¸šæœ¯è¯­åˆ—è¡¨
    """
    terms = set()
    for pattern in DOMAIN_TERM_PATTERNS:
        matches = re.findall(pattern, text, re.IGNORECASE)
        terms.update(m.strip() for m in matches if len(m.strip()) >= 2)
    
    # è¿‡æ»¤å¸¸è§è¯
    common_words = {'AI', 'OK', 'API', 'US', 'UK', 'EU', 'IT', 'ID'}
    terms = [t for t in terms if t.upper() not in common_words]
    
    return list(terms)[:5]  # æœ€å¤šè¿”å›5ä¸ª


def build_search_query(user_input: str, terms: list[str]) -> str:
    """
    æ„å»ºè½»é‡æœç´¢çš„æŸ¥è¯¢è¯­å¥ã€‚
    
    Args:
        user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
        terms: æå–çš„ä¸“ä¸šæœ¯è¯­
    
    Returns:
        æœç´¢æŸ¥è¯¢è¯­å¥
    """
    if terms:
        # ä½¿ç”¨ä¸“ä¸šæœ¯è¯­æ„å»ºæŸ¥è¯¢
        main_term = terms[0]
        return f"{main_term} drug research indications mechanism"
    else:
        # æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ ¸å¿ƒè¯
        # å»æ‰å¸¸è§è¯·æ±‚è¯
        cleaned = re.sub(r'(å¸®æˆ‘|è¯·|æœé›†|ç ”ç©¶|åˆ†æ|äº†è§£|æŸ¥æ‰¾|æ‰¾)', '', user_input)
        cleaned = cleaned.strip()[:50]  # é™åˆ¶é•¿åº¦
        if cleaned:
            return f"{cleaned} research overview"
    return ""


async def pre_clarification_search(
    user_input: str,
    terms: list[str],
    num_results: int = LIGHT_SEARCH_NUM_RESULTS
) -> Optional[dict]:
    """
    æ¾„æ¸…å‰çš„è½»é‡æœç´¢ï¼Œè·å–èƒŒæ™¯ä¿¡æ¯ã€‚
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        terms: ä¸“ä¸šæœ¯è¯­åˆ—è¡¨
        num_results: æœç´¢ç»“æœæ•°é‡
    
    Returns:
        æœç´¢ç»“æœå­—å…¸ï¼ŒåŒ…å« domain_context å’Œ verified_terms
    """
    query = build_search_query(user_input, terms)
    if not query:
        return None
    
    try:
        logger.info(f"ğŸ” è½»é‡æœç´¢: {query}")
        search_result = await web_search(query, num_results=num_results)
        
        return {
            "query": query,
            "domain_context": search_result,
            "verified_terms": terms,
            "has_results": bool(search_result and "æœªæ‰¾åˆ°" not in search_result)
        }
    except Exception as e:
        logger.warning(f"è½»é‡æœç´¢å¤±è´¥: {e}")
        return None


def should_do_pre_search(user_input: str, task_draft: dict) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œæ¾„æ¸…å‰æœç´¢ã€‚
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        task_draft: å½“å‰ä»»åŠ¡è‰ç¨¿
    
    Returns:
        True å¦‚æœéœ€è¦æœç´¢
    """
    # å¦‚æœä»»åŠ¡è‰ç¨¿å·²æœ‰æ˜ç¡®ç›®æ ‡ï¼Œä¸éœ€è¦æœç´¢
    if task_draft and task_draft.get("goal") and task_draft.get("research_focus"):
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸“ä¸šæœ¯è¯­
    terms = extract_domain_terms(user_input)
    if terms:
        return True
    
    # æ£€æŸ¥è¾“å…¥é•¿åº¦ - å¤ªçŸ­ä¸æœç´¢
    if len(user_input.strip()) < 10:
        return False
    
    # åŒ…å«ç ”ç©¶ç›¸å…³å…³é”®è¯
    research_keywords = ['ç ”ç©¶', 'è°ƒç ”', 'åˆ†æ', 'æœºåˆ¶', 'é¶ç‚¹', 'è¯ç‰©', 'ä¸´åºŠ', 'å¸‚åœº', 'ç«äº‰']
    if any(kw in user_input for kw in research_keywords):
        return True
    
    return False


def _extract_json(s: str) -> dict:
    """Extract JSON from agent output."""
    s = (s or "").strip()
    if s.startswith("{") and s.endswith("}"):
        return json.loads(s)
    a, b = s.find("{"), s.rfind("}")
    if a != -1 and b != -1 and b > a:
        return json.loads(s[a:b+1])
    raise ValueError(f"Clarifier did not return JSON: {s[:200]}")


def should_clarify(plan: Plan) -> bool:
    """
    Determine if clarification is needed based on plan.
    
    Args:
        plan: Plan from clarifier
    
    Returns:
        True if clarification is needed
    """
    # Hard boundary: must clarify
    if plan.confidence < 0.6:
        return True
    
    # Soft boundary: missing key info
    if 0.6 <= plan.confidence < 0.7:
        # Check if research_focus is missing or too few
        if not plan.task.research_focus or len(plan.task.research_focus) < 2:
            return True
    
    return False


def should_start_research(plan: Plan) -> bool:
    """
    Determine if research can start directly.
    
    Args:
        plan: Plan from clarifier
    
    Returns:
        True if can start research directly
    """
    return (
        plan.confidence >= 0.85 and
        plan.task.goal and
        len(plan.task.research_focus) >= 3 and
        plan.next_action != "VERIFY_TOPIC"
    )


async def assess_input(
    model: LitellmModel,
    messages: list[dict],
    task_draft: dict,
    enable_pre_search: bool = True
) -> Plan:
    """
    Assess user input and determine if clarification is needed.
    
    Args:
        model: LLM model for clarification
        messages: Conversation history
        task_draft: Current task draft
        enable_pre_search: Whether to enable pre-clarification search
    
    Returns:
        Plan with next_action and assessment
    """
    clarifier = build_clarifier(model)
    
    # Build context from messages
    context = ""
    user_input = ""
    conversation_summary = ""
    if messages:
        # Get last few messages for context
        recent_messages = messages[-5:] if len(messages) > 5 else messages
        context = "\n".join([
            f"{msg.get('role', 'user')}: {msg.get('content', '')}"
            for msg in recent_messages
        ])
        # Get latest user input
        for msg in reversed(messages):
            if msg.get('role') == 'user':
                user_input = msg.get('content', '')
                break
        
        # æ„å»ºå¯¹è¯æ‘˜è¦ï¼Œå¸®åŠ© LLM ç†è§£ä¸Šä¸‹æ–‡
        user_msgs = [m for m in messages if m.get('role') == 'user']
        if len(user_msgs) >= 2:
            first_msg = user_msgs[0].get('content', '')
            conversation_summary = f"ç”¨æˆ·æœ€åˆè¯·æ±‚: {first_msg}"
            if task_draft.get('project_info'):
                conversation_summary += f"\nç”¨æˆ·è¡¥å……çš„é¡¹ç›®ä¿¡æ¯: {task_draft['project_info']}"
            elif task_draft.get('pipeline_info'):  # å…¼å®¹æ—§å­—æ®µ
                conversation_summary += f"\nç”¨æˆ·è¡¥å……çš„é¡¹ç›®ä¿¡æ¯: {task_draft['pipeline_info']}"
            if task_draft.get('clarification_responses'):
                for resp in task_draft['clarification_responses']:
                    conversation_summary += f"\né—®: {resp.get('question', '')}\nç­”: {resp.get('answer', '')}"
    
    # Pre-clarification search (if enabled and appropriate)
    search_context = None
    if enable_pre_search and user_input and should_do_pre_search(user_input, task_draft):
        terms = extract_domain_terms(user_input)
        search_context = await pre_clarification_search(user_input, terms)
        if search_context:
            logger.info(f"âœ… è½»é‡æœç´¢å®Œæˆï¼Œå‘ç°æœ¯è¯­: {search_context.get('verified_terms', [])}")
    
    payload = {
        "messages": messages,
        "task_draft": task_draft or {},
        "context": context,
        "conversation_summary": conversation_summary,  # å¯¹è¯è„‰ç»œæ‘˜è¦
        "search_context": search_context,  # æ·»åŠ æœç´¢ä¸Šä¸‹æ–‡
        "schema_hint": {
            "next_action": "START_RESEARCH|NEED_CLARIFICATION|CONFIRM_PLAN|VERIFY_TOPIC",
            "task": {
                "goal": "string",
                "research_focus": ["string"]  # At least 3 for START_RESEARCH
            },
            "confidence": "0.0-1.0",
            "why": "string",
            "clarification": {
                "question": "string",
                "options": ["string"],
                "missing_info": "research_topic|research_focus|research_goal"
            },
            "assumptions": ["string"],
            "confirm_prompt": "string"
        }
    }
    
    result = await Runner.run(clarifier, json.dumps(payload, ensure_ascii=False))
    data = _extract_json(result.final_output or "")
    plan = Plan.model_validate(data)
    
    # Post-process: enforce decision logic
    if should_clarify(plan) and plan.next_action not in ["NEED_CLARIFICATION", "VERIFY_TOPIC"]:
        plan.next_action = "NEED_CLARIFICATION"
    
    if should_start_research(plan) and plan.next_action == "CONFIRM_PLAN":
        plan.next_action = "START_RESEARCH"
    
    return plan
